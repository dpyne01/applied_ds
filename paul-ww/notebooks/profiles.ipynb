{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Initial Cleaning of the District Profiles Dataset\n",
    "## General Description\n",
    " As part of my project in the Tufts class EM212: Applied Datascience, I will\n",
    " try to analyse two datasets:\n",
    " *   the District Profiles (2007 to 2017) Dataset, containing socio-demographic information about Hamburg's districts\n",
    " *   the Ground Value (1964 to 2017) Dataset, containing estimates of the ground value of real estate in Hamburg,\n",
    " on an individual property level in Euros / $m^2$\n",
    "\n",
    "## Information on the District Profiles Dataset\n",
    " The dataset consists of two parts and provided by [Statistik Nord](https://www.statistik-nord.de) at\n",
    " *   [Profiles 2007 to 2016](https://www.statistik-nord.de/fileadmin/Dokumente/Datenbanken_und_Karten/Stadtteilprofile/Stadtteilprofile-Berichtsjahre-2007-2016.xlsx)\n",
    " *   [Profiles 2017](https://www.statistik-nord.de/fileadmin/Dokumente/Datenbanken_und_Karten/Stadtteilprofile/StadtteilprofileBerichtsjahr2017.xlsx)\n",
    "\n",
    " In this notebook, I will import these Excel files into pandas dataframes and save\n",
    " them as pickle files for use in my data analysis notebook.\n",
    "\n",
    "## Heads Up:\n",
    " I will use the current path of this notebook to store and manipulate files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import pickle\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the original German column names used in the Excel sheet are not very easy to\n",
    "understand for non-Germans and are also very long and thus difficult to work with, I\n",
    "have created a dedicated Excel sheet ```column_titles.xlsx``` which maps the German column names to\n",
    "1.   each other, where they differ across years\n",
    "2.   an abbreviation of the English translation of the column names\n",
    "This way, each old German column will be matched to its German counterparts and then\n",
    "converted to an English name which I will use in the rest of the analysis.\n",
    "The sheet contains missing values for those features which were not observed in particular years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "coltitles = pd.read_excel(  \"../data/column_titles.xlsx\",\n",
    "                            encoding='utf-8')\n",
    "coltitles.sort_index().head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Profiles from 2007 to 2016\n",
    "\n",
    " I will import the 2007-2016 Excel file as a pandas DataFrame. Since the data is stored in sheets,\n",
    " the result is an associative array of DataFrames with the sheet names as the keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "dfs = pd.read_excel(    \"https://www.statistik-nord.de/fileadmin/Dokumente/Datenbanken_und_Karten/Stadtteilprofile/Stadtteilprofile-Berichtsjahre-2007-2016.xlsx\",\n",
    "                        None,\n",
    "                        skiprows = 3\n",
    "                    )\n",
    "dfs.keys() # keys\n",
    "dfs[\"Berichtsjahr_2016\"].head(5) # df for year 2016\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the year is contained in each key, I can infer the observation year from the sheet\n",
    "name (key) and add it to each sub-df as a new column called \"year\". I am defining functions\n",
    "here because I will have to apply these again to the second dataset (2017 profiles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def add_year_to_df(dfs):\n",
    "    \"\"\"Reads the year from the sheet name and adds it as a column\"\"\"\n",
    "    for key in dfs.keys():\n",
    "        year = key.replace(\" \", \"_\").split(\"_\")[1]\n",
    "        dfs[key][\"year\"] = year\n",
    "    return dfs\n",
    "\n",
    "dfs = add_year_to_df(dfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions help me match the columns of each DataFrame to the ones present in\n",
    "the coltitles DataFrame. I have had some problems with the built-in string matching\n",
    "functions that are part of pandas, so I decided to write some myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def tedious_match(col, string):\n",
    "    \"\"\"My own function that matches strings better than the built-in tools for some reason\"\"\"\n",
    "    for i in range(0, len(col)):\n",
    "        if col.iloc[i] == string:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def pairs(dfs, names, return_not_found):\n",
    "    \"\"\"Returns df containing pairs of old and new titles\"\"\"\n",
    "    result = {}\n",
    "    not_found = {}\n",
    "    for key in dfs.keys():\n",
    "        result[dfs[key].columns[0]] = \"district\" # first column doesn't have a title in the excel file\n",
    "        for old_title in dfs[key].columns:\n",
    "            for new_title in names.columns:\n",
    "                # assign the new column title to any column that is found in my mapping sheet\n",
    "                if old_title == names[new_title].any():\n",
    "                    result[old_title] = new_title\n",
    "                # I have had some problems with the built-in options, this one seems to work, but is slow!\n",
    "                elif tedious_match(names[new_title], old_title):\n",
    "                    result[old_title] = new_title\n",
    "                else:\n",
    "                    not_found[old_title] = \"not found\"\n",
    "    # convert output to a dataframe\n",
    "    assignment = pd.DataFrame(result, index=[0])\n",
    "    result = assignment.transpose()\n",
    "    result.reset_index(inplace=True)\n",
    "    result.columns = [\"old\", \"new\"]\n",
    "    if return_not_found:\n",
    "        not_found = pd.DataFrame(not_found, index=[0])\n",
    "        not_found = not_found.transpose()\n",
    "        not_found.reset_index(inplace=True)\n",
    "        not_found.columns = [\"old\", \"new\"]\n",
    "        return result, not_found\n",
    "    return result\n",
    "\n",
    "def list_all_titles(df):\n",
    "    \"\"\"Returns a df holding all unique column titles of df\"\"\"\n",
    "    all_titles = []\n",
    "    for key in df.keys():\n",
    "        all_titles.extend([title for title in df[key].columns])\n",
    "    all_titles = pd.DataFrame(all_titles)\n",
    "    all_titles.columns = [\"old\"]\n",
    "    return all_titles.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "def find_frame(df, s):\n",
    "    \"\"\"Returns a list of the subframes of df a string s occurs in\"\"\"\n",
    "    occurs_in = []\n",
    "    for key in df.keys():\n",
    "        if s in df[key].columns:\n",
    "            occurs_in.append(key)\n",
    "    return occurs_in\n",
    "\n",
    "def overwrite_old_colnames(dfs, pairs):\n",
    "    \"\"\"Changes the old column name to the new one\"\"\"\n",
    "    result = dfs\n",
    "    for key in result.keys():\n",
    "        for old_title in result[key]:\n",
    "            try:\n",
    "                new_title = pairs.loc[pairs[\"old\"] == old_title, \"new\"].values[0]\n",
    "                result[key].rename(columns={old_title:new_title}, inplace=True)\n",
    "            except:\n",
    "                continue\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important that all columns are assigned a new column name. To do this, I check if all column titles\n",
    "that are present in the Data sheet were found in my prepared reassignment sheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "all_old_column_titles = list_all_titles(dfs)\n",
    "proposed_assignments = pairs(dfs, coltitles, False)\n",
    "# # perform outer join to receive list of all old titles, indicator makes clear which assignments are missing\n",
    "merged = all_old_column_titles.merge(proposed_assignments, on=\"old\", how=\"outer\", indicator=True)\n",
    "# list those titles that have not yet been mapped to a new title and show which sheet they occur in\n",
    "missing_assignment = merged[merged.isnull().any(axis=1)]\n",
    "missing_assignment[\"occurence\"] = missing_assignment[\"old\"].apply(lambda x: find_frame(dfs, x))\n",
    "missing_assignment[[\"old\", \"occurence\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since I did not specify a new column name for ```year``` in coltitles, I did not expect\n",
    "to find a match with this column. It seems like all other column have been matched\n",
    "with a counterpart from my coltitles mapping sheet! These are the proposed assignments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "proposed_assignments.head() # the first column (district) doesn't have a title in the source\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the matching process went well, I can now overwrite the old column titles with\n",
    "the new, common English ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "reassigned_dfs = overwrite_old_colnames(dfs, proposed_assignments)\n",
    "titles_post_assignment = list_all_titles(reassigned_dfs)\n",
    "titles_post_assignment.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like all column titles have been updated successfully!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### District Profiles for the year 2017\n",
    "\n",
    " The newest dataset is from 2018 and holds the observations for the year 2017.\n",
    " It comes in another excel sheet that I have to import separately:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "df_2017 = pd.read_excel( \"https://www.statistik-nord.de/fileadmin/Dokumente/Datenbanken_und_Karten/Stadtteilprofile/StadtteilprofileBerichtsjahr2017.xlsx\",\n",
    "                        None,\n",
    "                        skiprows = 3\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps taken for the district profiles from 2007 to 2016 should also\n",
    "be applicable for the 2017 data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "df_2017 = add_year_to_df(df_2017) # add year data\n",
    "all_old_column_titles_2017 = list_all_titles(df_2017)\n",
    "proposed_assignments_2017 = pairs(df_2017, coltitles, False)\n",
    "# check for missing assignments\n",
    "merged_2017 = all_old_column_titles_2017.merge(proposed_assignments_2017, on=\"old\", how=\"outer\", indicator=True)\n",
    "missing_assignment_2017 = merged_2017[merged_2017.isnull().any(axis=1)]\n",
    "missing_assignment_2017[\"occurence\"] = missing_assignment_2017[\"old\"].apply(lambda x: find_frame(df_2017, x))\n",
    "proposed_assignments_2017.head() # proposed reassignments\n",
    "missing_assignment_2017[[\"old\", \"occurence\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the only unmatched column is ```year````, for which I do not expect to find a match.\n",
    "I proceed with reassigning the new names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# reassign column titles\n",
    "reassigned_df_2017 = overwrite_old_colnames(df_2017, proposed_assignments_2017)\n",
    "titles_post_assignment_2017 = list_all_titles(reassigned_df_2017)\n",
    "titles_post_assignment_2017\n",
    "# one last check\n",
    "still_unmatched_2017 = titles_post_assignment_2017.merge(all_old_column_titles_2017, on=\"old\")\n",
    "still_unmatched_2017[\"occurence\"] =  still_unmatched_2017[\"old\"].apply(lambda x: find_frame(df_2017, x))\n",
    "still_unmatched_2017 # this is expected, as I do not have a \"year\" column in my mapping sheet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This too seems to have worked. The 2017 data is now ready to be merged with the rest\n",
    "of the district profiles dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the two District Profile Datasets\n",
    " This too seems to have worked. In the next step, I have to combine all of the excel\n",
    " sheets of the 2007 to 2016 profiles into one single dataframe, which I can then merge\n",
    " with the 2017 DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "combined_2007_2016 = pd.concat(reassigned_dfs, axis=0, sort=False) # concatenate all DataFrames in the list into one\n",
    "combined_2007_2016.reset_index(inplace=True)\n",
    "combined_2007_2016.drop(columns=[\"level_0\", \"level_1\"], inplace=True)\n",
    "combined_2007_2016.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still have to append the dataset from 2017:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data = combined_2007_2016.append(reassigned_df_2017[\"Stadtteilprofile 2017\"], sort=False)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DataFrame ```data``` now holds all instances from both of the district profile Excel files.\n",
    "I will pickle the dataset to have a backup and to be able to skip the cells above in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.to_pickle(\"../data/district_profiles_2007_2017.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset\n",
    " If the cells above have been skipped, the data can be read from the pickle file at this point.\n",
    " Let's take a quick look at the columns of the combined dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/district_profiles_2007_2017.pkl\") # read the stored dataset\n",
    "data.info()\n",
    "data.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some rows appear to hold averages across some or all districts. It makes sense to exclude these for now,\n",
    "as this information can easily be recovered from the individual districts and would only skew results.\n",
    "In Hamburg, there are no districts or larger administrative units with more than 1,000,000 inhabitants,\n",
    "so any \"districts\" with those numbers must be aggregates. Here is a plot showing the current distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "sns.boxplot(x=\"year\", y=\"population\", data=data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no districts or \"Bezirke\" with more than 1,000,000 inhabitants, these can only be city-wide aggregates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data[\"district\"].loc[data[\"population\"] > 1000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also no districts with a population of more than 100,000 inhabitants, these can only be wards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data[\"district\"].loc[data[\"population\"] > 100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will exclude all of these aggregate values, since I will conduct my analysis on a district level. Furthermore,\n",
    "these aggregates should be easy to recover by averaging the district data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts = data[data.population < 100000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The updated population distribution looks much more plausible now, with Rahlstedt as the outlier.\n",
    "It is the most populous district with more than 90,000 inhabitants, followed by Billstedt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "sns.relplot(x=\"year\", y=\"population\", data=districts)\n",
    "districts[[\"year\", \"district\", \"population\"]].loc[districts[\"population\"] > 60000].tail(2) # the largest two districts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ground values dataset with which I will eventually merge this one,\n",
    "the districts St. Pauli and St. Georg are\n",
    "spelled with the \".\" after the \"St\". In order to prevent merge conflicts\n",
    "later, I will change all occurences of \"St  Pauli\" and \"St  Georg\" in this\n",
    "dataset to \"St. Pauli\" and \"St. Georg\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts.district.replace(\"St  Pauli\", \"St. Pauli\", inplace=True)\n",
    "districts.district.replace(\"St  Georg\", \"St. Georg\", inplace=True)\n",
    "districts.district.replace(\"Altenwerder und Moorburg\", \"Moorburg und Altenwerder\", inplace=True) #switch these around\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the columns do not seem to have the right datatype set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "non_float64 = pd.DataFrame(districts.select_dtypes(exclude=\"number\").dtypes).reset_index()\n",
    "non_float64.columns = [\"column\", \"dtype\"]\n",
    "non_float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will reassign the columns new datatypes that make more sense to me. Since I\n",
    "will have to perform this action quite often, I have packaged pandas' ```to_numeric```\n",
    "function in a function of my own:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def force_col_to_num(col, df=districts):\n",
    "    \"\"\"Convert a column col in dataframe df to numeric values\"\"\"\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Districts should be categorical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts[\"district\"] = districts[\"district\"].astype(\"category\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Years can be cast down to integers to save space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts[\"year\"] = districts[\"year\"].astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns have only been encoded as ```objects```\n",
    "because they contain the odd empty string. I will force convert\n",
    "these to float64:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "force_col_to_num(\"unemp_15_25\")\n",
    "force_col_to_num(\"unemp_15_25_rel\")\n",
    "force_col_to_num(\"unemp_55_65\")\n",
    "force_col_to_num(\"unemp_55_65_rel\")\n",
    "force_col_to_num(\"unemp_sgb2\")\n",
    "force_col_to_num(\"rec_sgb2\")\n",
    "force_col_to_num(\"rec_sgb2_rel\")\n",
    "force_col_to_num(\"hholds_shared\")\n",
    "force_col_to_num(\"tax_revenue\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cdu column (share of the vote for CDU party) contains some values that are part of strings.\n",
    "I need to extract them and convert to column to a numeric one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts[\"cdu\"].unique()\n",
    "\n",
    "def extract_nnd(num):\n",
    "    \"\"\"Extracts the first two digits followed by a point and another digit in a string\"\"\"\n",
    "    result = re.findall('(\\d\\d.\\d)', str(num))\n",
    "    if not result:\n",
    "        return float(\"NaN\")\n",
    "    return result[0]\n",
    "\n",
    "# apply the function to the column\n",
    "districts[\"cdu\"] = districts.cdu.apply(extract_nnd)\n",
    "districts[\"cdu\"].unique() # seems to have worked!\n",
    "\n",
    "force_col_to_num(\"cdu\") # convert the column to numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is repeated in the other columns that hold the share of the vote for the different parties.\n",
    "I will apply the same function to these columns as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts[\"spd\"] = districts.spd.apply(extract_nnd)\n",
    "force_col_to_num(\"spd\")\n",
    "\n",
    "districts[\"greens\"] = districts.greens.apply(extract_nnd)\n",
    "force_col_to_num(\"greens\")\n",
    "\n",
    "districts[\"left\"] = districts.left.apply(extract_nnd)\n",
    "force_col_to_num(\"left\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining columns seem not to contain any non-numeric values except for NaNs.\n",
    "I will force convert them to numeric data as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "force_col_to_num(\"real_estate_price_mean\")\n",
    "force_col_to_num(\"house_price_mean\")\n",
    "force_col_to_num(\"condo_price_mean\")\n",
    "force_col_to_num(\"doctors_resident\")\n",
    "force_col_to_num(\"rec_sgb2_u15\")\n",
    "force_col_to_num(\"rec_sgb2_u15_rel\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the old and the new datatypes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "non_float64[\"new dtype\"] = [districts[col].dtype for col in non_float64[\"column\"]]\n",
    "non_float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that this should conclude the cleaning of the most obvious flaws in the dataset.\n",
    "I will store dataset as a pickle file that I will use in my data analysis later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "districts.info()\n",
    "districts.describe()\n",
    "districts.to_pickle(\"../data/cleaned_district_profiles_2007_2017.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
