{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Initial Cleaning of the Ground Value Dataset\n",
    "## General Description\n",
    " As part of my project in the Tufts class EM212: Applied Datascience, I will\n",
    " try to analyse two datasets:\n",
    " *   the District Profiles (2007 to 2017) Dataset, containing socio-demographic information about Hamburg's districts\n",
    " *   the Ground Value (1964 to 2017) Dataset, containing estimates of the ground value of real estate in Hamburg,\n",
    " on an individual property level in Euros / $m^2$\n",
    "\n",
    "## Information on the Ground Value Dataset\n",
    " Each year, the city of Hamburg publishes rough estimates of the value of\n",
    " real estate on property level. The so-called \"Bodenrichtwert\" (abbr. BRW, \"reference ground value\")\n",
    " is the value of a particular property broken down to a $1m^2$ plot of land at a certain location (measured in coordinates).\n",
    " The calculation of the BRW takes the following features into account:\n",
    " *   location (x,y coordinates)\n",
    " *   suitability for development and use (categorical)\n",
    " *   current development / use (categorical)\n",
    " *   size of the plot\n",
    "\n",
    " A recent, detailed description of the data and the features used in the calculation is available\n",
    " [here (in German)](https://www.hamburg.de/contentblob/10917486/73e458aa8a5e46f772eacb2b00b4c393/data/d-brw-erlaeuterungen-2017.pdf).\n",
    " Additional information is available [here (also in German)](https://www.hamburg.de/bsw/grundstueckswerte/7916004/bodenrichtwert-erlaeuterungen/)\n",
    "\n",
    " The City of Hamburg makes this data available through its [Open Data Portal](http://suche.transparenz.hamburg.de/dataset/bodenrichtwerte-fur-hamburg6?forceWeb=true).\n",
    " Unfortunately, the data is split up into multiple CSV files, each representing\n",
    " one year of observations. In order to work with this dataset, I will download\n",
    " the individual CSVs, import them into pandas DataFrames, merge these together\n",
    " and save the result as a CSV file.\n",
    "\n",
    "## Heads Up:\n",
    " In order to download and process and store the data, about 4GB of free disk space is required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import requests\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sys\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only execute the next cell if you would like to download the whole dataset.\n",
    "The cell will take a long time to complete, as the server does not seem to offer\n",
    "fast download speeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "with open(\"urls.txt\") as file:\n",
    "    urls = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "def download(urls):\n",
    "    '''Downloads the files specified in list urls'''\n",
    "    for url in urls:\n",
    "        successful = 0\n",
    "        count = len(urls)\n",
    "        filename = url.rsplit('/', 1)[1]\n",
    "        print(\"Downloading \" + filename + \" (\" + str(successful) + \"/\" + str(count) + \" completed)...\")\n",
    "        r = requests.get(url, allow_redirects=True)\n",
    "        open(filename, 'wb').write(r.content)\n",
    "        print(\"Done.\")\n",
    "        current += 1\n",
    "\n",
    "download(urls) # start the download\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming that the downloaded CSV files are now present in the working directory,\n",
    "I will import them into pandas DataFrames and join these into a larger one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "files = glob.glob(\"*.csv\") # aggregate all csv files for reading\n",
    "files = sorted(files) # sort files to start with the smallest one\n",
    "files\n",
    "\n",
    "# import the data from the CSV files\n",
    "dfs = []\n",
    "for file in files:\n",
    "    print(\"Reading file \" + file + \"...\")\n",
    "    dfs.append(pd.read_csv( file,\n",
    "                            sep=\"|\",\n",
    "                            header=0,\n",
    "                            encoding=\"ISO-8859-1\" # this encoding seems to be correct\n",
    "                            ))\n",
    "print(\"Concatening dataframes...\")\n",
    "data = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "print(\"Done.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Data Survey and Cleaning\n",
    " Let's take a look at the data. It seems like there are many columns where a\n",
    " common ```dtype``` could not be recognized. Because of this, the dataset takes\n",
    " up much more storage space than it would if the ```dtypes``` were set properly.\n",
    " This should be the first priority before saving the joined dataset to save storage\n",
    " space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.head(5)\n",
    "data.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the [dataset metadata information sheet](https://www.hamburg.de/contentblob/8025502/a106909f7f9c11b8d90312a99a5b62c8/data/d-vboris-modellbeschreibung.pdf),\n",
    "I have the following list of\n",
    "variable names and their description. I will first identify those that I will\n",
    "not be using in my data analysis and can thus drop:\n",
    "\n",
    "| Variable Name | Description                                      | ```dtype``` / drop |\n",
    "|---------------|--------------------------------------------------|--------------------|\n",
    "| GESL          | Ward (code)                                      | drop               |\n",
    "| GENA          | Ward (name)                                      | drop               |\n",
    "| GASL          | Surveying institution (code)                     | drop               |\n",
    "| GABE          | Surveying institution (name)                     | drop               |\n",
    "| GENU          | Land Survey Registry (code)                      | drop               |\n",
    "| GEMA          | Land Survey Registry (name)                      | drop               |\n",
    "| ORTST         | District (name)                                  | category           |\n",
    "| WNUM          | Ground Value Number (code)                       | drop               |\n",
    "| BRW           | Ground Value                                     | float              |\n",
    "| STAG          | Observation / Survey Date                        | date (dd.mm.yyyy)  |\n",
    "| BRKE          | kind of ground value                             | integer            |\n",
    "| BEDW          | \"Requirements value\"                             | drop               |\n",
    "| PLZ           | zip code                                         | integer            |\n",
    "| BASBE         | base map reference                               | drop               |\n",
    "| BASMA         | base map scale                                   | drop               |\n",
    "| YWERT         | BRW geo-reference (east)                         | integer            |\n",
    "| XWERT         | BRW geo-reference (north)                        | integer            |\n",
    "| BEZUG         | coordinate system used                           | drop               |\n",
    "| ENTW          | current state of development                     | category           |\n",
    "| BEIT          | development state according to taxes             | category           |\n",
    "| NUTA          | kind of current usage                            | category           |\n",
    "| ERGNUTA       | extension of NUTA                                | category           |\n",
    "| BAUW          | current building type                            | category           |\n",
    "| GEZ           | number of floors                                 | float              |\n",
    "| WGFZ          | number of floors relevant for BRW                | float              |\n",
    "| GRZ           | site coverage factor                             | float              |\n",
    "| BMZ           | cubic index                                      | float              |\n",
    "| FLAE          | site area  in $m^2$                              | float              |\n",
    "| GTIE          | site depth  in meters                            | float              |\n",
    "| GBREI         | site width in meters                             | float              |\n",
    "| ERVE          | accessibility                                    | category           |\n",
    "| VERG          | zoning city development information              | category           |\n",
    "| VERF          | state of remediation                             | category           |\n",
    "| YVERG         | coordinate of city planning measure (east)       | drop               |\n",
    "| XVERG         | coordinate of city planning measure (north)      | drop               |\n",
    "| BOD           | kind of ground                                   | float              |\n",
    "| ACZA          | index value of agricultural land                 | drop               |\n",
    "| GRZA          | index value of agricultural land                 | drop               |\n",
    "| AUFW          | historical reference for reforestation           | drop               |\n",
    "| WEER          | accessible roads for agriculture / forestry      | drop               |\n",
    "| KOORWERT      | geo-reference polygon for BRW zone               | drop               |\n",
    "| KOORVERF      | geo-reference polygon for city planning measures | drop               |\n",
    "| BEM           | notes                                            | drop               |\n",
    "\n",
    "As a first step, I will mark the selected columns to be dropped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [    \"GESL\", \"GENA\", \"GASL\", \"GABE\", \"GENU\", \"GEMA\",\n",
    "                    \"WNUM\", \"BEDW\", \"BASBE\", \"BASMA\", \"BEZUG\", \"YVERG\",\n",
    "                    \"XVERG\", \"ACZA\", \"GRZA\", \"AUFW\", \"WEER\", \"KOORWERT\",\n",
    "                    \"KOORVERF\", \"BEM\" ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I will list all columns that were not detected as being numerical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "non_numerical = pd.DataFrame(data.select_dtypes(exclude=\"number\").dtypes).reset_index()\n",
    "non_numerical\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I know that there are some column that are empty or that only contain one value.\n",
    "I wrote a function that should check each column for this condition and then\n",
    "save its only value and mark it to be dropped from the main dataset.\n",
    "\n",
    "The following columns appear to be empty or contain only one value, which has been saved in\n",
    "´´´saved_info´´´. They can thus be dropped from the main dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "saved_info = {} # dictionary for info that might be useful, but does not need to be in dataframe\n",
    "\n",
    "def save_info_and_mark(drop=cols_to_drop, save=saved_info, df=data):\n",
    "    \"\"\"If a column only contains one unique value, save it into save and mark to column to be dropped\"\"\"\n",
    "    for column in df:\n",
    "        unique_values = df[column].unique()\n",
    "        if len(unique_values) == 1:\n",
    "            save[column] = df[column].unique()[0]\n",
    "            if column not in drop:\n",
    "                drop.append(column)\n",
    "    return save, drop\n",
    "\n",
    "# iterate over all columns in non-numerical and check for columns containing only one value,\n",
    "# save its content and mark it to be dropped\n",
    "saved_info, cols_to_drop = save_info_and_mark()\n",
    "\n",
    "almost_empty = pd.DataFrame(saved_info, index=[0]).transpose().reset_index() # show the stored content\n",
    "almost_empty.columns = [\"column\", \"only value\"]\n",
    "almost_empty\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now time to actually drop the columns that I earmarked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.drop(columns=cols_to_drop, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at the remaining columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# list the remaining non-numerical columns\n",
    "non_numerical = pd.DataFrame(data.select_dtypes(exclude=\"number\").dtypes).reset_index()\n",
    "non_numerical\n",
    "\n",
    "unique = {}\n",
    "for column in non_numerical[\"index\"]: # list the unique values of the remaining non numerical columns\n",
    "    unique[column] = data[column].unique()\n",
    "unique\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are a few more columns that I can drop or convert:\n",
    "*   ORTST contains the district names, I will make it categorical\n",
    "*   STAG holds the date the real estate evaluation was done for, so convert it to datetime\n",
    "*   PLZ holds ZIP codes and can safely be converted to integer\n",
    "*   there appear to be some categorical variables, I will convert them to categorical format\n",
    "    - ENTW\n",
    "    - NUTA\n",
    "    - ERGNUTA\n",
    "    - BAUW\n",
    "    - VERG\n",
    "    - VERF\n",
    "*   FREI seems very messy, I will try to work without it for now\n",
    "*   BRZNAME holds only a few addresses, I will not need it for my analysis\n",
    "*   LUMNUM contains a reference to the PDF holding the corresponding metadata, should be safe to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "cols_to_drop = [\"FREI\", \"BRZNAME\", \"LUMNUM\"] # drop these columns\n",
    "data.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "data.ORTST = data.ORTST.astype(\"category\") # convert ORTST to category\n",
    "data.STAG = pd.to_datetime(data.STAG, format=\"%d.%m.%Y\") # convert STAG to datetime\n",
    "data.PLZ = pd.to_numeric(data.PLZ, downcast=\"integer\", errors=\"coerce\") # convert PLZ to numerical, it contains some strings\n",
    "# convert BASBE, ENTW, NUTA, ERGNUTA, BAUW, VERG, VERF, BEM to categorical\n",
    "to_convert = [\"ENTW\", \"NUTA\", \"ERGNUTA\", \"BAUW\", \"VERG\", \"VERF\"]\n",
    "for var in to_convert:\n",
    "    data[var] = data[var].astype(\"category\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there are any columns that only contain missing values. These can be dropped\n",
    "as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "for column in data:\n",
    "    if data[column].isnull().all():\n",
    "        data.drop(columns=column, inplace=True)\n",
    "        print(\"Dropped column \" + column)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like there are some issues with the German Umlauts in ORTST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.ORTST.loc[data.ORTST.str.contains(\"Ã\")].unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem seems to be wrong encoding:\n",
    "- \"ß\" has been replaced with \"Ã\"\n",
    "- \"ü\" has been replaced with \"Ã¼\"\n",
    "- \"ö\" has been replaced with \"Ã¶\"\n",
    "I will try to undo these changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "def revert_encoding(s):\n",
    "    if \"Ã¶\" in s:\n",
    "        return s.replace(\"Ã¶\", \"ö\")\n",
    "    if \"Ã¼\" in s:\n",
    "        return s.replace(\"Ã¼\", \"ü\")\n",
    "    if \"Ã\\x9f\" in s:\n",
    "        return s.replace(\"Ã\\x9f\", \"ß\")\n",
    "    return s\n",
    "\n",
    "data.ORTST = data.ORTST.apply(revert_encoding) # apply the new function to the ORSTS column\n",
    "data.ORTST = data.ORTST.astype(\"category\") # convert ORTST to category again\n",
    "pd.DataFrame(item for item in data.ORTST.unique()) # this looks much better!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The somewhat reduced dataset now looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.head(5)\n",
    "data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic cleaning done, I can now store the dataset as a pickle file.\n",
    "Since it is still rather large, I will compress it after pickling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data.to_pickle(\"../data/joined_ground_values.pkl.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset for merging\n",
    " Assuming the data has been saved into ```../data/joined_ground_values.pkl``` before,\n",
    " it is possible to continue from this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "data = pd.read_pickle(\"../data/joined_ground_values.pkl.zip\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the other dataset (district profiles) is on a district-year-level, I need to collapse this dataset\n",
    "over districts and years. For this step, it is important to know which variables\n",
    "can be collapsed in a meaningful way. For example, it does not make sense to\n",
    "collapse the zip codes using an average. Since my analysis will focus on the ground value,\n",
    "I will first collapse only this column by district (```ORTST```) and year (```STAG```). In\n",
    "the collapsed dataset, I will use the new column name ```GV``` for the ground value.\n",
    "Since the average of the ground values might be skewed by expensive clusters within\n",
    "districts, I will also include the median and some descriptive statistics while collapsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Paul/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>GV_mean</th>\n",
       "      <th>GV_median</th>\n",
       "      <th>GV_std</th>\n",
       "      <th>GV_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>district</th>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Allermöhe</th>\n",
       "      <th>1964</th>\n",
       "      <td>13.420000</td>\n",
       "      <td>12.78</td>\n",
       "      <td>1.280000</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>16.876000</td>\n",
       "      <td>17.90</td>\n",
       "      <td>1.402170</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1975</th>\n",
       "      <td>21.476000</td>\n",
       "      <td>23.01</td>\n",
       "      <td>2.285822</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1977</th>\n",
       "      <td>30.678000</td>\n",
       "      <td>30.68</td>\n",
       "      <td>3.616852</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>63.058889</td>\n",
       "      <td>51.13</td>\n",
       "      <td>26.689191</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  GV_mean  GV_median     GV_std  GV_count\n",
       "district  year                                           \n",
       "Allermöhe 1964  13.420000      12.78   1.280000       4.0\n",
       "          1973  16.876000      17.90   1.402170       5.0\n",
       "          1975  21.476000      23.01   2.285822       5.0\n",
       "          1977  30.678000      30.68   3.616852       5.0\n",
       "          1980  63.058889      51.13  26.689191       9.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_collapse = data[[\"ORTST\", \"STAG\", \"BRW\"]]\n",
    "to_collapse.columns = [\"district\", \"year\", \"GV\"]\n",
    "to_collapse[\"year\"] = to_collapse[\"year\"].dt.year # preserve only the year for now\n",
    "\n",
    "collapsed = to_collapse.groupby([\"district\", \"year\"]).mean()\n",
    "collapsed.columns = [\"GV_mean\"]\n",
    "collapsed[\"GV_median\"] = to_collapse.groupby([\"district\", \"year\"]).median()\n",
    "collapsed[\"GV_std\"] = to_collapse.groupby([\"district\", \"year\"]).std()\n",
    "collapsed[\"GV_count\"] = to_collapse.groupby([\"district\", \"year\"]).count()[\"GV\"]\n",
    "collapsed.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like everything went well! It is now time to store the finished dataset and\n",
    "continue with the data analysis (see ```data-analysis``` notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "inputHidden": false,
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "collapsed.to_pickle(\"../data/collapsed_ground_values.pkl\") # holds collapsed data across all years"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
